{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdbdfb6",
   "metadata": {},
   "source": [
    "Code \"merge uimacas\" - permet de fusionner les fichiers contenants l'adjudication et les fichiers contenants les niveaux de pre-parsing (pos tagging, morph etc.) pour effectuer des statistiques dessus par la suite. Donne en sortie un dossier contenant les fichiers uima cas à utiliser pour les stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from cassis import *\n",
    "import shutil\n",
    "\n",
    "#TODO iterate over whole folder, unzip every sub folder annotation and curation before merging\n",
    "\n",
    "# filepath annotation --> ./annotation/*.conllu/INITIAL_CAS.xmi\n",
    "# check whether typesystem is the same for initial cas and annotation files\n",
    "# filepath curation --> ./curation/*.conllu/CURATION_USER\n",
    "# for each iteration when merging, *.conllu must be the same in both files\n",
    "\n",
    "#pour fr project='scolinter---fran_ais-2024-3-18500502075926473488'\n",
    "project = 'italien'\n",
    "#root folder = './francais_tout/francese-export/nome progetto'\n",
    "root_folder_ann='./italiano-export/'+project+'/annotation/'\n",
    "\n",
    "parent_folders=[]\n",
    "\n",
    "# Iterate through the root folder\n",
    "for subdir, dirs, files in os.walk(root_folder_ann):\n",
    "    for file in files:\n",
    "        if file == \"INITIAL_CAS.zip\":\n",
    "            # Construct the full path to the zip file\n",
    "            zip_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Extract the parent folder name\n",
    "            parent_folder_name = os.path.basename(os.path.dirname(zip_path))\n",
    "            parent_folder_name = parent_folder_name[:-7]\n",
    "            # Append to the list for future use\n",
    "            parent_folders.append(parent_folder_name)\n",
    "\n",
    "            # Create an output directory to extract the contents\n",
    "            output_dir = os.path.join(subdir, parent_folder_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Unzip the file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(output_dir)\n",
    "\n",
    "            # print(f\"Extracted {file} from {parent_folders} to {output_dir}\")\n",
    "            \n",
    "\n",
    "# The parent folder names are stored in the list\n",
    "# print(\"Parent folders processed:\", parent_folders)\n",
    "\n",
    "#root folder\n",
    "root_folder_cure='./italiano-export/'+project+'/curation/'\n",
    "\n",
    "parent_folders=[]\n",
    "\n",
    "# Iterate through the root folder\n",
    "for subdir, dirs, files in os.walk(root_folder_cure):\n",
    "    for file in files:\n",
    "        if file == \"CURATION_USER.zip\":\n",
    "            # Construct the full path to the zip file\n",
    "            zip_path = os.path.join(subdir, file)\n",
    "\n",
    "            # Extract the parent folder name\n",
    "            parent_folder_name = os.path.basename(os.path.dirname(zip_path))\n",
    "            parent_folder_name = parent_folder_name[:-7]\n",
    "            # Append to the list for future use\n",
    "            parent_folders.append(parent_folder_name)\n",
    "\n",
    "            # Create an output directory to extract the contents\n",
    "            output_dir = os.path.join(subdir, \"curation_\"+parent_folder_name)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Unzip the file\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(output_dir)\n",
    "\n",
    "            # print(f\"Extracted {file} from {parent_folders} to {output_dir}\")\n",
    "            \n",
    "\n",
    "# The parent folder names are stored in the list\n",
    "# print(\"Parent folders processed:\", parent_folders)\n",
    "\n",
    "#prendere il typesystem della curation per selezionare le Menzioni e copiarle sul file INITIAL CAS durante il merging\n",
    "#with open('./francese-export/'+project+'/curation/NORM-EC-CE1-2015-5-D1-S3026-V1.conllu/curation_NORM-EC-CE1-2015-5-D1-S3026-V1/TypeSystem.xml', 'rb') as f:\n",
    "\n",
    "with open('./italiano-export/'+project+'/curation/NORM_CE1_8.conllu/curation_NORM_CE1_8/TypeSystem.xml', 'rb') as f:\n",
    "    typesystem_cure = load_typesystem(f)\n",
    "    \n",
    "for subdir in os.listdir(root_folder_ann):\n",
    "    subfolder_path = os.path.join(root_folder_ann, subdir)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        contained_folders = [os.path.join(subfolder_path, folder) for folder in os.listdir(subfolder_path) if os.path.isdir(os.path.join(subfolder_path, folder))]\n",
    "        for folder in contained_folders:\n",
    "            with open (folder+'/INITIAL_CAS.xmi', 'rb') as f:\n",
    "                cas_ann = load_cas_from_xmi(f, typesystem=typesystem_cure)\n",
    "            folder = os.path.basename(folder)\n",
    "            with open ('./italiano-export/'+project+'/curation/'+folder+'.conllu/curation_'+folder+'/CURATION_USER.xmi', 'rb') as g:\n",
    "                cas_cure = load_cas_from_xmi(g, typesystem=typesystem_cure)\n",
    "\n",
    "\n",
    "            # Select the specific annotation type to transfer\n",
    "            annotation_type = typesystem_cure.get_type('webanno.custom.Mentions')  # Replace with the annotation name\n",
    "\n",
    "            # 1. Remove the existing 'Mention' annotations in the target CAS\n",
    "            for annotation in cas_ann.select(annotation_type.name):\n",
    "                cas_ann.remove(annotation)\n",
    "\n",
    "\n",
    "            # 2. Copy the new 'Mention' annotations from the source CAS\n",
    "            for annotation in cas_cure.select(annotation_type.name):\n",
    "                # Create a new annotation in the target CAS\n",
    "                new_annotation = cas_ann.typesystem.get_type(annotation_type.name)(\n",
    "                    begin=annotation.begin,\n",
    "                    end=annotation.end\n",
    "                )\n",
    "\n",
    "                # Copy all features from the source annotation\n",
    "                for feature in annotation_type.all_features:\n",
    "                    if feature.name != 'sofa':  # Skip the sofa feature\n",
    "                        value = getattr(annotation, feature.name, None)\n",
    "                        if value is not None:\n",
    "                            setattr(new_annotation, feature.name, value)\n",
    "\n",
    "                # Add the new annotation to the target CAS\n",
    "                cas_ann.add(new_annotation)\n",
    "\n",
    "            # Save the updated target CAS to a file\n",
    "            f = 'C:/Users/matil/OneDrive/Documenti/doctorat_travaux/stats/italien/'+folder+'.xmi'\n",
    "            print(f)\n",
    "            cas_ann.to_xmi(f)\n",
    "\n",
    "#TODO copy TypeSystem.xml dans dossier output\n",
    "# Copy the file\n",
    "typesystem = './italiano-export/'+project+'/curation/NORM_CE1_8.conllu/curation_NORM_CE1_8/TypeSystem.xml'\n",
    "destination_folder='./italien'\n",
    "shutil.copy(typesystem, destination_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb0f54",
   "metadata": {},
   "source": [
    "code pour le traitement des UIMA CAS et obtention des stats et sorties intérmediaires\n",
    "prends en entrée un folder contenants des uima cas 1.0 obtenus à l'aide du bloc de code précedent (merge uimacas) et permet d'obtenir des csv intermediaires avec une ligne par mention annoté et informations détaillées sur chaque mention annotée (version adjudication) - version pour le français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900df10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ONE FILE VERSION : \n",
    "Created on Thu Jan  9 17:26:37 2025\n",
    "\n",
    "@author: matil\n",
    "\n",
    "Le brouillon initial de ce code a été réalisé en utilisant ChatGPT, puis il a été modifié\n",
    "selon les nécessités de mon travail de thèse.\n",
    "\n",
    "dernière version des stats pour annotations INCEpTION\n",
    "\n",
    "- nb tokens (avec, sans ponctuation)\n",
    "- nb de mentions avec étiquettes (mention chevauchées comptées une fois)\n",
    "- distance entre mentions\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "from cassis import *\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "##load TypeSystem - contains the annotation layers used in the code - tjrs le meme\n",
    "with open('./francais_tout/francais/TypeSystem.xml', 'rb') as f:\n",
    "    typesystem = load_typesystem(f)\n",
    "\n",
    "#CONSTANTS name of annotation layers that we use further - always the same\n",
    "token_type_name = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token'\n",
    "mention_type_name = 'webanno.custom.Mentions'\n",
    "morph='de.tudarmstadt.ukp.dkpro.core.api.lexmorph.type.morph.MorphologicalFeatures'\n",
    "\n",
    "\n",
    "token_type = typesystem.get_type(token_type_name)\n",
    "mention_type = typesystem.get_type(mention_type_name)\n",
    "morph_type = typesystem.get_type(morph)\n",
    "###############################################################################\n",
    "\n",
    "def load_cas_xmi(file_path):\n",
    "    \"\"\"\n",
    "    Load a CAS XMI file using cassis\n",
    "    Args:\n",
    "        file_path - XMI file path\n",
    "    Returns:\n",
    "        cas - CAS object containing the annotations\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)  \n",
    "        #lenient=True - Leniency helps bypass MetaData errors if any\n",
    "    return cas\n",
    "\n",
    "###############################################################################\n",
    "# NB TOKENS\n",
    "#count nb of tokens for text (1) and nb tokens without ponctuation for text (2)\n",
    "\n",
    "def count_tokens(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    nb_tokens = len(tokens)\n",
    "    print(f\"longueur du texte nb tokens : {nb_tokens}\")\n",
    "    return nb_tokens\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# (2) nb tokens without punctuation : filter tokens by POS tag\n",
    "def count_tokens_nopunct(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).PosValue != 'PUNCT']\n",
    "    nb_tokens_nopunct = len(non_punct_tokens)\n",
    "    print(f\"longueur du texte nb tokens sans PUNCT : {nb_tokens_nopunct}\")    \n",
    "    return nb_tokens_nopunct\n",
    "\n",
    "def count_tokens_nouns(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).PosValue == 'NOUN']\n",
    "    nb_tokens_nopunct = len(non_punct_tokens)\n",
    "    print(f\"Tokens qui sont des NOUNS : {nb_tokens_nopunct}\")    \n",
    "    return nb_tokens_nopunct\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def extract_mention_details(cas, token_type, mention_type):\n",
    "    \"\"\"\n",
    "    Extracts details of mentions, including: mention text, begin index, end index (caracters not tokens), \n",
    "    mention tag, nb of tokens in mention, POS of each token in the mention, POS of each token filtering PUNCT tokens\n",
    "\n",
    "    Arguments:\n",
    "        cas: The CAS object containing annotations.\n",
    "        token_type_name: The type name for tokens (e.g., 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token').\n",
    "        mention_type_name: The type name for mentions (e.g., 'webanno.custom.Mentions').\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each sub-list contains mention text, begin index, end index, mention tag, \n",
    "        and POS of each token in the mention expression.\n",
    "    \"\"\"\n",
    "\n",
    "    mentions = list(cas.select(mention_type))\n",
    "    tokens = list(cas.select(token_type))\n",
    "    \n",
    "    mention_details = []\n",
    "    \n",
    "    # text, begin and end character, mention tag\n",
    "    for mention in mentions:\n",
    "        mention_text = mention.get_covered_text()\n",
    "        begin_index = mention.begin\n",
    "        end_index = mention.end\n",
    "        mention_tag = mention.mention\n",
    "         \n",
    "\n",
    "        # POS of tokens within the mention\n",
    "        pos_list = []\n",
    "        morph_list = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.begin >= begin_index and token.end <= end_index:\n",
    "                pos_annotation = getattr(token, 'pos', None)\n",
    "                morph_annotation = getattr(token, 'morph', None)\n",
    "                if pos_annotation:\n",
    "                    pos_list.append(pos_annotation.PosValue)\n",
    "                if morph_annotation:\n",
    "                    morph_list.append(morph_annotation.value)\n",
    "                    \n",
    "            \n",
    "\n",
    "        # POS of tokens within the mention span no punctuation\n",
    "        pos_list_no_punct = []\n",
    "        morph_list_no_punct = []\n",
    "        for token in tokens:\n",
    "            if token.begin >= begin_index and token.end <= end_index:\n",
    "                pos_annotation = getattr(token, 'pos', None)\n",
    "                morph_annotation = getattr(token, 'morph', None)\n",
    "                if pos_annotation.PosValue != 'PUNCT' :\n",
    "                    pos_list_no_punct.append(pos_annotation.PosValue)\n",
    "                if morph_annotation:\n",
    "                    morph_list_no_punct.append(morph_annotation.value)\n",
    "                        \n",
    "        \n",
    "        #TODO add morphological details \n",
    "\n",
    "        mention_details.append([mention_text, begin_index, end_index, mention_tag, \n",
    "                                pos_list, morph_list, \n",
    "                                pos_list_no_punct, morph_list_no_punct])\n",
    "    \n",
    "    return mention_details\n",
    "\n",
    "\n",
    "def create_mentions_df(mention_details):\n",
    "    \"\"\"\n",
    "    Parameters :\n",
    "    ----------\n",
    "    mention_details : A list of lists, where each sub-list contains mention text, \n",
    "    begin index, end index, mention tag, and POS of each token in the mention expression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame containing mention text, begin and end caracter, mention tag (flattened\n",
    "    for overlapping mentions), POS list with and without punctuation, lenght of mention in tokens with/\n",
    "    without punctuation, \n",
    "\n",
    "    \"\"\"\n",
    "    #transform datas into DataFrame and add mention len --> one DataFrame for each text here\n",
    "    columns=['mention', 'begin', 'end', 'tag', 'POS', 'morph', 'POSno', 'morphNoPunct']\n",
    "    df = pd.DataFrame(mention_details, columns=columns)\n",
    "    \n",
    "    df['mentionLen'] = df['POS'].apply(len)\n",
    "    df['mentionLenNoPunct'] = df['POSno'].apply(len)\n",
    "    \n",
    "    # Group by 'begin' and 'end' and aggregate the 'tag' column - count only once overlapping mentions\n",
    "    ment_details = (\n",
    "        df.groupby(['begin', 'end'], as_index=False)\n",
    "        .agg({\n",
    "            'mention': 'first',   # Take the first mention (or customize as needed)\n",
    "            'tag': list,          # Combine tags into a list\n",
    "            'POS': 'first',       # Take the first POS (or customize)\n",
    "            'morph': 'first', \n",
    "            'mentionLen': 'first', # Take the first mentionLen (or customize)\n",
    "            'POSno': 'first',\n",
    "            'morphNoPunct': 'first', \n",
    "            'mentionLenNoPunct' : 'first'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # flatten tag lists - tag for overlapping mentions -> list of tags, one row\n",
    "    # normalize tags order by sorting alphabetically\n",
    "    #ment_details['tag'] = ment_details['tag'].apply(lambda tags: list(set(tags)))\n",
    "    ment_details['tag'] = ment_details['tag'].apply(lambda tags: sorted(list(set(tags))))\n",
    "\n",
    "    return ment_details\n",
    "\n",
    "###V2 take into account overlapping mentions in both languages\n",
    "def calcul_distance(df):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame qui contient \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['distance'] = 0  # Initialize distance column\n",
    "        \n",
    "    for i in range(1, len(df)): #début deuxième mention\n",
    "        curr_begin = df.loc[i, 'tokenBegin']\n",
    "        curr_end = df.loc[i, 'tokenEnd']\n",
    "        prev_begin = df.loc[i - 1, 'tokenBegin']\n",
    "        prev_end = df.loc[i - 1, 'tokenEnd']\n",
    "\n",
    "        # Case 1: Distant spans (-1 to take into account current token)\n",
    "        # si début mention courante est après la fin de la mention précedente -> \n",
    "        if curr_begin > prev_end:\n",
    "        #distance rajouté dans ligne de mention précedente\n",
    "        #c'est la distance entre cette mention et la suivante \n",
    "            df.loc[i, 'distance'] = curr_begin - prev_end - 1\n",
    "\n",
    "        ## Case 2: Mentions imbriquées\n",
    "        ###si le début est le meme, la distance depuis ment précédente est de 0\n",
    "        ## ((son) chat)\n",
    "        if curr_begin == prev_begin and curr_end > prev_end :\n",
    "            df.loc[i, 'distance'] = 0\n",
    "        \n",
    "        # # Case 3: Smaller span inside larger span \n",
    "        # (ita) il [loro] amico\n",
    "        # come calcolare ?\n",
    "        elif curr_begin > prev_begin and curr_end < prev_end:\n",
    "            df.loc[i, 'distance'] = 0  # indiquer avec val diff ?\n",
    "\n",
    "\n",
    "        # # Case 4: Same ending, smaller span\n",
    "        # elif curr_end == prev_end and curr_begin > prev_begin:\n",
    "        #     df.loc[i, 'distance'] = -3  # Arbitrary value for identification\n",
    "\n",
    "\n",
    "        #distance de la première mention depuis le début du texte\n",
    "        df.loc[0, 'distance'] = df.loc[0, 'tokenBegin']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def determine_begin_token(cas, ment_details) :\n",
    "    \"\"\"\n",
    "    contient calcul distance pour rajouter au df la colonne de distance entre mentions\n",
    "    le valeur indique la distance entre la fin de la mention précédente et le début \n",
    "    de la mention suivante - première mention 0 ? dernière mention ?\n",
    "    \n",
    "    permet de trouver le token de début et de fin des mentions puis\n",
    "    utilise calcul_distance pour rajouter col distance au df \n",
    "\n",
    "    Parameters    \n",
    "    cas : TYPE\n",
    "    ment_details : pandas DataFrame avec détails des mentions \n",
    "    (indice début et fin, étiquette etc.)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ment_details : pandas DataFrame en entrée avec colonne des distances en plus\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    ##compter distance entre tokens\n",
    "    tokens = list(cas.select(token_type))\n",
    "    #mentions = list(cas.select(mention_type))\n",
    "    \n",
    "    #filter out punctuation tokens from the text\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).PosValue != 'PUNCT']\n",
    "    \n",
    "    #à partir des mentions, on calcule le token de début de chaque mention\n",
    "    #listes vides pour indices de début et de fin de la mention    \n",
    "    mention_indices_begin = []\n",
    "    mention_indices_end = []\n",
    "    \n",
    "    # Match tokens with mentions and track indices\n",
    "    for mention in ment_details.itertuples():  # Iterate over rows in ment_details df\n",
    "        begin_index = None\n",
    "        end_index = None\n",
    "        #iterate over tokens in text\n",
    "        for i, token in enumerate(non_punct_tokens):\n",
    "            if token.begin == mention.begin and begin_index is None:\n",
    "                begin_index = i\n",
    "            if token.end == mention.end and end_index is None:\n",
    "                end_index = i\n",
    "        \n",
    "        mention_indices_begin.append(begin_index)\n",
    "        mention_indices_end.append(end_index)\n",
    "        \n",
    "    if len(mention_indices_begin) != len(ment_details):\n",
    "        raise ValueError(\"Length mismatch: 'mention_indices_begin' and 'ment_details' row count do not match.\")\n",
    "\n",
    "    if len(mention_indices_end) != len(ment_details):\n",
    "        raise ValueError(\"Length mismatch: 'mention_indices_end' and 'ment_details' row count do not match.\")\n",
    "    \n",
    "    ment_details['tokenBegin'] = mention_indices_begin\n",
    "    ment_details['tokenEnd'] = mention_indices_end\n",
    "    \n",
    "    ment_details = calcul_distance(ment_details)\n",
    "    \n",
    "    return ment_details\n",
    "\n",
    "def position_in_chain(df, tag_column=\"tag\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    associer index à chaque mention correspondant à sa position dans sa chaine\n",
    "    ex. deuxième mention de la chaine \"cat\" = 2\n",
    "    \"\"\"\n",
    "    tag_counts = {}\n",
    "    occurrences = []\n",
    "    \n",
    "    for tags in df[tag_column]:\n",
    "        tag_tuple = tuple(tags) if isinstance(tags, list) else (tags,)\n",
    "        \n",
    "        if tag_tuple not in tag_counts:\n",
    "            tag_counts[tag_tuple] = 1\n",
    "        else:\n",
    "            tag_counts[tag_tuple] += 1\n",
    "        \n",
    "    occurrences.append(tag_counts[tag_tuple])\n",
    "\n",
    "    df[\"tag_occurrences\"] = occurrences\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mentions_by_tag(cas, mention_type_name):\n",
    "    \"\"\"\n",
    "    Regroupe les mentions par étiquette. \n",
    "    Overlapping mentions are counted once for each tag\n",
    "    TODO add overlapping mentions option in dictionary ? how ?\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cas : TYPE\n",
    "        DESCRIPTION.\n",
    "    mention_type_name : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ment_by_tag : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    mention_type = typesystem.get_type(mention_type_name)\n",
    "    mentions = list(cas.select(mention_type))\n",
    "    \n",
    "    # Organize mentions by tag\n",
    "    ment_by_tag = {}\n",
    "    for mention in mentions:\n",
    "        tag = mention.mention\n",
    "        if tag not in ment_by_tag:\n",
    "            ment_by_tag[tag] = []\n",
    "        ment_by_tag[tag].append(mention)\n",
    "\n",
    "    return ment_by_tag\n",
    "\n",
    "def mentions_per_entity(data_dict, key_order):\n",
    "    \"\"\"\n",
    "    Returns the lengths of the value lists in a dictionary, following a specific order of keys.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dict (dict): The input dictionary where keys map to lists of objects.\n",
    "    - key_order (list): A list specifying the desired order of keys.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lengths corresponding to the key order, with 0 or None for missing keys.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for key in key_order:\n",
    "        if key in data_dict:\n",
    "            result.append(len(data_dict[key]))\n",
    "        else:\n",
    "            result.append(0)  # Change to None if None is preferred\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_tag_occurrence_column(df, tag_column=\"tag\"):\n",
    "    #nuñéroter les mentions meme tag pour ordre\n",
    "    tag_counts = {}\n",
    "    occurrences = []\n",
    "\n",
    "    for tags in df[tag_column]:\n",
    "        tag_tuple = tuple(sorted(tags)) if isinstance(tags, list) else (tags,)\n",
    "\n",
    "        if tag_tuple not in tag_counts:\n",
    "            tag_counts[tag_tuple] = 1\n",
    "        else:\n",
    "            tag_counts[tag_tuple] += 1\n",
    "        \n",
    "        occurrences.append(tag_counts[tag_tuple])\n",
    "    \n",
    "    df[\"tag_occurrences\"] = occurrences\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to convert tags containing commas\n",
    "def convert_tags(tag_str):\n",
    "    # Replace commas with '+'\n",
    "    a = tag_str.replace(\"[\",\"\")\n",
    "    b = a.replace(\"]\",\"\")\n",
    "    c = b.replace(\" \",\"\")\n",
    "    d = c.replace(\"'\",\"\")\n",
    "    return d.replace(',', '+')\n",
    "\n",
    "\n",
    "#before recomposing apply calcul distance on each group        \n",
    "# Function to calculate distances\n",
    "def calcul_interdistance(df):\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "    df = df.reset_index(drop=True)  # Reset index to ensure sequential order\n",
    "    df['interdistance'] = 0  # Initialize distance column\n",
    "\n",
    "    for i in range(1, len(df)):  # Start from the second row\n",
    "    \n",
    "        curr_begin = df.loc[i, 'tokenBegin']\n",
    "        curr_end = df.loc[i, 'tokenEnd']\n",
    "        prev_begin = df.loc[i - 1, 'tokenBegin']\n",
    "        prev_end = df.loc[i - 1, 'tokenEnd']\n",
    "\n",
    "        # Case 1: Distant spans\n",
    "        if curr_begin > prev_end:\n",
    "            df.loc[i, 'interdistance'] = curr_begin - prev_end - 1\n",
    "\n",
    "        # Case 2: Overlapping mentions (nested)\n",
    "        elif curr_begin == prev_begin and curr_end > prev_end:\n",
    "            df.loc[i, 'interdistance'] = 0\n",
    "\n",
    "        # Case 3: Smaller span inside a larger span\n",
    "        elif curr_begin > prev_begin and curr_end < prev_end:\n",
    "            df.loc[i, 'interdistance'] = 0  \n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def unify_csv_files(folder_path, filter_substring=None):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in the specified folder into a single Pandas DataFrame.\n",
    "    Uses the header from the first file and skips the first row for subsequent files.\n",
    "    Adds a column with a substring extracted from the filename.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        filter_substring (str, optional): Substring to filter files by name. \n",
    "                                          Only files containing this substring will be included.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Unified DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(folder_path)\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    \n",
    "    # Apply optional filter\n",
    "    if filter_substring:\n",
    "        csv_files = [f for f in csv_files if filter_substring in f]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No files matched the criteria or the folder is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    header = None  # To store the header from the first file\n",
    "    for i, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "                \n",
    "        try:\n",
    "            if i == 0:\n",
    "                # Read the first file normally, and set its header\n",
    "                df = pd.read_csv(file_path)\n",
    "                header = df.columns  # Save header from the first file\n",
    "            else:\n",
    "                # For subsequent files, skip the first row\n",
    "                df = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "                df.columns = header  # Assign header to the current DataFrame\n",
    "            \n",
    "            # Add the extracted value as a new column\n",
    "            df['Source'] = file\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    unified_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Step 1: Replace '0' in 'distance' with NaN where 'Index' is 0\n",
    "#    unified_df.loc[unified_df['Unnamed: 0'] == 0, 'distance'] = unified_df.loc[unified_df['Unnamed: 0'] == 0, 'distance'].replace(0, np.nan)\n",
    "    return unified_df\n",
    "\n",
    "\n",
    "#tagset pour francais et italien complet\n",
    "key_order = [\n",
    "            'cat', 'witch', 'wolf', 'robot', 'ext1', 'ext2', 'ext3', 'ext4', \\\n",
    "            'ext5', 'ext6', 'ext7', 'ext8', 'ext9', 'cat1', 'cat2', 'cat3', \\\n",
    "            'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'robot1', 'robot2', \\\n",
    "            'robot3', 'robot4', 'robot5', 'witch1', 'witch2', 'witch3', \\\n",
    "            'wolf1', 'wolf2', 'wolf3'\n",
    "            ]\n",
    "\n",
    "################  MAIN PROGRAM  ###############################################\n",
    "folder_path = os.path.dirname('./francais_tout/francais/')\n",
    "\n",
    "all_res=[]\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    results = []\n",
    "    if filename.endswith('.xmi'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        filename = os.path.splitext(filename)[0]\n",
    "        # Load CAS\n",
    "        cas = load_cas_xmi(file_path)\n",
    "        results.append(filename)\n",
    "        \n",
    "        #compter nb tokens\n",
    "        toks = count_tokens(cas)\n",
    "        results.append(toks)\n",
    "        toks_no_punct = count_tokens_nopunct(cas)\n",
    "        results.append(toks_no_punct)\n",
    "        \n",
    "        #creer dataframe 1 per texte qui contient tout détail de mentions\n",
    "        #obtenir un folder avec un fichier csv par texte avec le détail des mentions\n",
    "        mention_details = extract_mention_details(cas, token_type, mention_type)\n",
    "        mention_dataframe = create_mentions_df(mention_details)\n",
    "        mention_dataframe_2 = determine_begin_token(cas, mention_dataframe)\n",
    "        mention_dataframe_3 = add_tag_occurrence_column(mention_dataframe_2)\n",
    "        os.makedirs('./stats_fr_v3/', exist_ok=True)\n",
    "        mention_dataframe_3.to_csv(\"./stats_fr_v3/\"+filename+\".csv\", \",\", encoding=\"utf-8\")\n",
    "        \n",
    "        # TODO dataframe avec interdistance meme entité calculée\n",
    "        #todo integrer dans uima cas tt\n",
    "        # Group by 'tag', apply function, and recompose\n",
    "        # Apply the function to convert tags in the 'tag' column\n",
    "        mention_dataframe_3['tag'] = mention_dataframe_3['tag'].astype(str).apply(convert_tags)\n",
    "        grouped_df = mention_dataframe_3.groupby('tag', sort=False)  # Keep original order\n",
    "        processed_groups = [calcul_interdistance(group) for _, group in grouped_df]\n",
    "        recomposed_df = pd.concat(processed_groups).reset_index(drop=True)\n",
    "        os.makedirs('./stats_fr_v3_interdistance/', exist_ok=True)\n",
    "        recomposed_df.to_csv(\"./stats_fr_v3_interdistance/\"+filename+\".csv\", \",\", encoding=\"utf-8\")\n",
    "\n",
    "        #trouver les tags à rajouter au df \n",
    "        ment_tags = mentions_by_tag(cas,mention_type_name)    \n",
    "        output = mentions_per_entity(ment_tags, key_order)\n",
    "        results = results + output\n",
    "        prev_header = ['texte', 'nbTok', 'nbTokNoPunct'] + key_order\n",
    "        header = prev_header\n",
    "        all_res.append(results)\n",
    "\n",
    "\n",
    "#dataframe with nbTok (with/noPunct) and mentions per character in text\n",
    "#TODO add how many mentions are plural and what characters are involved?\n",
    "\n",
    "header = ['texte', 'nbTok', 'nbTokNoPunct'] + key_order\n",
    "#csv unique avec tout le corpus + colonne nom fichier en dernier\n",
    "res = pd.DataFrame(all_res, columns = header)\n",
    "#droppa colonne che contengono solo degli zero ok\n",
    "res.replace(0, np.nan).dropna(axis=1,how=\"all\")\n",
    "res.to_csv(\"./corpus_francais_v3.csv\", \",\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# FR folder = './stats_fr_v3/'\n",
    "df = unify_csv_files(\"./stats_fr_v3_interdistance/\")  # un seul csv depuis folder\n",
    "print(df)\n",
    "df.to_csv(\"./toutes_mentions_corpus_detail_fr.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "# folder = './stats_ita_interdistance/'\n",
    "# df2 = unify_csv_files(folder)  # un seul csv depuis folder\n",
    "# print(df2)\n",
    "# df2.to_csv(\"./toutes_mentions_corpus_detail_interdistance_fr.csv\", sep=\",\", encoding=\"utf8\")\n",
    "nb_texts = len(res['texte'])\n",
    "nb_tokens = res['nbTokNoPunct'].sum()\n",
    "\n",
    "# Supponiamo che il tuo DataFrame sia df\n",
    "\n",
    "# Count occurrences of each tag per source\n",
    "df_pivot = df.groupby(['Source', 'tag']).size().unstack(fill_value=0)\n",
    "\n",
    "# # Numero di colonne che contengono almeno un valore diverso da zero\n",
    "nb_characters = (df_pivot != 0).any(axis=0).sum()\n",
    "\n",
    "nb_mentions = len(df)\n",
    "\n",
    "\n",
    "# Numero di valori maggiori o uguali a 3 nelle colonne dalla quarta in poi\n",
    "chaines = (df_pivot >= 3).sum().sum()\n",
    "\n",
    "# Numero di valori uguali a 2 nelle colonne dalla quarta in poi\n",
    "anaphores = (df_pivot == 2).sum().sum()\n",
    "\n",
    "# Numero di valori uguali a 1 nelle colonne dalla quarta in poi\n",
    "singletons = (df_pivot == 1).sum().sum()\n",
    "\n",
    "# Step 1: Create a new column with the sum of integer values per row\n",
    "df_pivot['all_mentions'] = df_pivot.select_dtypes(include='number').sum(axis=1)\n",
    "\n",
    "df_pivot = df_pivot.reset_index()\n",
    "# Step 2: Sum all values in 'row_sum' where 'texte' contains 'CE1'\n",
    "total_sum_ce1 = df_pivot.loc[df_pivot['Source'].str.contains('CE1', na=False), 'all_mentions'].sum()\n",
    "print(\"Total sum where 'Source' contains 'CE1':\", total_sum_ce1)\n",
    "\n",
    "# Step 2: Sum all values in 'row_sum' where 'texte' contains 'CE1'\n",
    "total_sum_ce2 = df_pivot.loc[df_pivot['Source'].str.contains('CE2', na=False), 'all_mentions'].sum()\n",
    "print(\"Total sum where 'Source' contains 'CE2':\", total_sum_ce2)\n",
    "\n",
    "\n",
    "##change\n",
    "folder = './stats_fr_v3_interdistance/'\n",
    "df2 = unify_csv_files(folder)  # un seul csv depuis folder\n",
    "print(df2)\n",
    "df2.to_csv(\"./toutes_mentions_corpus_detail_interdistance_fr.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "# Group by 'Source' and 'tag' and perform both operations\n",
    "result = df2.groupby(['Source', 'tag'], as_index=False).agg({\n",
    "    'distance': 'sum',\n",
    "    'tag_occurrences': 'max',\n",
    "})\n",
    "# Creazione della nuova colonna 'interdistance'\n",
    "result['interdistance'] = result['distance'] / result['tag_occurrences']\n",
    "len_moy_chaines = result['tag_occurrences'].mean()\n",
    "result.to_csv(\"./interdistance_fr.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "ce1_len_moy = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].mean()\n",
    "ce2_len_moy = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].mean()\n",
    "\n",
    "len_max_chaines = result['tag_occurrences'].max()\n",
    "ce1_len_max = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].max()\n",
    "ce2_len_max = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].max()\n",
    "\n",
    "# moy_chaines_texte = \n",
    "# ce1_len_moy = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].mean()\n",
    "# ce2_len_moy = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].mean()\n",
    "\n",
    "\n",
    "txt = \"./res_fr.txt\"\n",
    "with open(txt, \"w\") as file:\n",
    "    file.write(f\"Nb of texts: {nb_texts}\\n\")\n",
    "    file.write(f\"Nb of tokens: {nb_tokens}\\n\")\n",
    "    file.write(f\"Nb of characters: {nb_characters}\\n\")\n",
    "    file.write(f\"Nb of mentions: {nb_mentions}\\n\")\n",
    "    file.write(f\"Nb of mentions CE1 : {total_sum_ce1}\\n\")\n",
    "    file.write(f\"Nb of mentions CE2 : {total_sum_ce2}\\n\")\n",
    "    \n",
    "    file.write(f\"Chaines  >= 3: {chaines}\\n\")\n",
    "    file.write(f\"Anaphores  = 2: {anaphores}\\n\")\n",
    "    file.write(f\"Singletons = 1: {singletons}\\n\")\n",
    "    file.write(f\"Len moyenne chaines {len_moy_chaines}\\n\")\n",
    "    file.write(f\"Len moyenne chaines CE1 {ce1_len_moy}\\n\")\n",
    "    file.write(f\"Len moyenne chaines CE2 {ce2_len_moy}\\n\")\n",
    "    file.write(f\"Len max chaines {len_max_chaines}\\n\")\n",
    "    file.write(f\"Len max chaines CE1 {ce1_len_max}\\n\")\n",
    "    file.write(f\"Len max chaines CE2 {ce2_len_max}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte {}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte CE1 {}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte CE2 {}\\n\")\n",
    "    file.write(f\"Tot.: {chaines+anaphores+singletons}\\n\")\n",
    "    file.write(f\"\")\n",
    "print(f\"File {txt} has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f692e",
   "metadata": {},
   "source": [
    "code pour le traitement des UIMA CAS et obtention des stats et sorties intérmediaires\n",
    "prends en entrée un folder contenants des uima cas 1.0 obtenus à l'aide du bloc de code précedent (merge uimacas) et permet d'obtenir des csv intermediaires avec une ligne par mention annoté et informations détaillées sur chaque mention annotée (version adjudication) - version pour l'italien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from cassis import *\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "##load TypeSystem - contains the annotation layers used in the code - tjrs le meme\n",
    "with open('./italien/TypeSystem.xml', 'rb') as f:\n",
    "    typesystem = load_typesystem(f)\n",
    "\n",
    "#CONSTANTS name of annotation layers that we use further - always the same\n",
    "token_type_name = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token'\n",
    "mention_type_name = 'webanno.custom.Mentions'\n",
    "morph='de.tudarmstadt.ukp.dkpro.core.api.lexmorph.type.morph.MorphologicalFeatures'\n",
    "\n",
    "\n",
    "token_type = typesystem.get_type(token_type_name)\n",
    "mention_type = typesystem.get_type(mention_type_name)\n",
    "morph_type = typesystem.get_type(morph)\n",
    "###############################################################################\n",
    "\n",
    "def load_cas_xmi(file_path):\n",
    "    \"\"\"\n",
    "    Load a CAS XMI file using cassis\n",
    "    Args:\n",
    "        file_path - XMI file path\n",
    "    Returns:\n",
    "        cas - CAS object containing the annotations\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        cas = load_cas_from_xmi(f, typesystem=typesystem)  \n",
    "        #lenient=True - Leniency helps bypass MetaData errors if any\n",
    "    return cas\n",
    "\n",
    "###############################################################################\n",
    "# NB TOKENS\n",
    "#count nb of tokens for text (1) and nb tokens without ponctuation for text (2)\n",
    "\n",
    "def count_tokens(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    nb_tokens = len(tokens)\n",
    "    print(f\"longueur du texte nb tokens : {nb_tokens}\")\n",
    "    return nb_tokens\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# (2) nb tokens without punctuation : filter tokens by POS tag\n",
    "def count_tokens_nopunct(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).coarseValue != 'PUNCT']\n",
    "    nb_tokens_nopunct = len(non_punct_tokens)\n",
    "    print(f\"longueur du texte nb tokens sans PUNCT : {nb_tokens_nopunct}\")    \n",
    "    return nb_tokens_nopunct\n",
    "\n",
    "def count_tokens_nouns(cas):\n",
    "    tokens = list(cas.select(token_type))\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).coarseValue == 'NOUN']\n",
    "    nb_tokens_nopunct = len(non_punct_tokens)\n",
    "    print(f\"Tokens qui sont des NOUNS : {nb_tokens_nopunct}\")    \n",
    "    return nb_tokens_nopunct\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def extract_mention_details(cas, token_type, mention_type):\n",
    "    \"\"\"\n",
    "    Extracts details of mentions, including: mention text, begin index, end index (caracters not tokens), \n",
    "    mention tag, nb of tokens in mention, POS of each token in the mention, POS of each token filtering PUNCT tokens\n",
    "\n",
    "    Arguments:\n",
    "        cas: The CAS object containing annotations.\n",
    "        token_type_name: The type name for tokens (e.g., 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token').\n",
    "        mention_type_name: The type name for mentions (e.g., 'webanno.custom.Mentions').\n",
    "\n",
    "    Returns:\n",
    "        A list of lists, where each sub-list contains mention text, begin index, end index, mention tag, \n",
    "        and POS of each token in the mention expression.\n",
    "    \"\"\"\n",
    "\n",
    "    mentions = list(cas.select(mention_type))\n",
    "    tokens = list(cas.select(token_type))\n",
    "    \n",
    "    mention_details = []\n",
    "    \n",
    "    # text, begin and end character, mention tag\n",
    "    for mention in mentions:\n",
    "        mention_text = mention.get_covered_text()\n",
    "        begin_index = mention.begin\n",
    "        end_index = mention.end\n",
    "        mention_tag = mention.mention\n",
    "         \n",
    "\n",
    "        # POS of tokens within the mention\n",
    "        pos_list = []\n",
    "        morph_list = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.begin >= begin_index and token.end <= end_index:\n",
    "                pos_annotation = getattr(token, 'pos', None)\n",
    "                morph_annotation = getattr(token, 'morph', None)\n",
    "                if pos_annotation:\n",
    "                    pos_list.append(pos_annotation.coarseValue)\n",
    "                if morph_annotation:\n",
    "                    morph_list.append(morph_annotation.value)\n",
    "                    \n",
    "            \n",
    "\n",
    "        # POS of tokens within the mention span no punctuation\n",
    "        pos_list_no_punct = []\n",
    "        morph_list_no_punct = []\n",
    "        for token in tokens:\n",
    "            if token.begin >= begin_index and token.end <= end_index:\n",
    "                pos_annotation = getattr(token, 'pos', None)\n",
    "                morph_annotation = getattr(token, 'morph', None)\n",
    "                if pos_annotation.PosValue != 'PUNCT' :\n",
    "                    pos_list_no_punct.append(pos_annotation.coarseValue)\n",
    "                if morph_annotation:\n",
    "                    morph_list_no_punct.append(morph_annotation.value)\n",
    "                        \n",
    "        \n",
    "        #TODO add morphological details \n",
    "\n",
    "        mention_details.append([mention_text, begin_index, end_index, mention_tag, \n",
    "                                pos_list, morph_list, \n",
    "                                pos_list_no_punct, morph_list_no_punct])\n",
    "    \n",
    "    return mention_details\n",
    "\n",
    "\n",
    "def create_mentions_df(mention_details):\n",
    "    \"\"\"\n",
    "    Parameters :\n",
    "    ----------\n",
    "    mention_details : A list of lists, where each sub-list contains mention text, \n",
    "    begin index, end index, mention tag, and POS of each token in the mention expression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame containing mention text, begin and end caracter, mention tag (flattened\n",
    "    for overlapping mentions), POS list with and without punctuation, lenght of mention in tokens with/\n",
    "    without punctuation, \n",
    "\n",
    "    \"\"\"\n",
    "    #transform datas into DataFrame and add mention len --> one DataFrame for each text here\n",
    "    columns=['mention', 'begin', 'end', 'tag', 'POS', 'morph', 'POSno', 'morphNoPunct']\n",
    "    df = pd.DataFrame(mention_details, columns=columns)\n",
    "    \n",
    "    df['mentionLen'] = df['POS'].apply(len)\n",
    "    df['mentionLenNoPunct'] = df['POSno'].apply(len)\n",
    "    \n",
    "    # Group by 'begin' and 'end' and aggregate the 'tag' column - count only once overlapping mentions\n",
    "    ment_details = (\n",
    "        df.groupby(['begin', 'end'], as_index=False)\n",
    "        .agg({\n",
    "            'mention': 'first',   # Take the first mention (or customize as needed)\n",
    "            'tag': list,          # Combine tags into a list\n",
    "            'POS': 'first',       # Take the first POS (or customize)\n",
    "            'morph': 'first', \n",
    "            'mentionLen': 'first', # Take the first mentionLen (or customize)\n",
    "            'POSno': 'first',\n",
    "            'morphNoPunct': 'first', \n",
    "            'mentionLenNoPunct' : 'first'\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # flatten tag lists - tag for overlapping mentions -> list of tags, one row\n",
    "    # normalize tags order by sorting alphabetically\n",
    "    #ment_details['tag'] = ment_details['tag'].apply(lambda tags: list(set(tags)))\n",
    "    ment_details['tag'] = ment_details['tag'].apply(lambda tags: sorted(list(set(tags))))\n",
    "\n",
    "    \n",
    "    return ment_details\n",
    "\n",
    "###V2 take into account overlapping mentions in both languages\n",
    "def calcul_distance(df):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame qui contient \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['distance'] = 0  # Initialize distance column\n",
    "        \n",
    "    for i in range(1, len(df)): #début deuxième mention\n",
    "        curr_begin = df.loc[i, 'tokenBegin']\n",
    "        curr_end = df.loc[i, 'tokenEnd']\n",
    "        prev_begin = df.loc[i - 1, 'tokenBegin']\n",
    "        prev_end = df.loc[i - 1, 'tokenEnd']\n",
    "\n",
    "        # Case 1: Distant spans (-1 to take into account current token)\n",
    "        # si début mention courante est après la fin de la mention précedente -> \n",
    "        if curr_begin > prev_end:\n",
    "        #distance rajouté dans ligne de mention précedente\n",
    "        #c'est la distance entre cette mention et la suivante \n",
    "            df.loc[i, 'distance'] = curr_begin - prev_end - 1\n",
    "\n",
    "        ## Case 2: Mentions imbriquées\n",
    "        ###si le début est le meme, la distance depuis ment précédente est de 0\n",
    "        ## ((son) chat)\n",
    "        if curr_begin == prev_begin and curr_end > prev_end :\n",
    "            df.loc[i, 'distance'] = 0\n",
    "        \n",
    "        # # Case 3: Smaller span inside larger span \n",
    "        # (ita) il [loro] amico\n",
    "        # come calcolare ?\n",
    "        elif curr_begin > prev_begin and curr_end < prev_end:\n",
    "            df.loc[i, 'distance'] = 0  # indiquer avec val diff ?\n",
    "\n",
    "\n",
    "        # # Case 4: Same ending, smaller span\n",
    "        # elif curr_end == prev_end and curr_begin > prev_begin:\n",
    "        #     df.loc[i, 'distance'] = -3  # Arbitrary value for identification\n",
    "\n",
    "\n",
    "        #distance de la première mention depuis le début du texte\n",
    "        df.loc[0, 'distance'] = df.loc[0, 'tokenBegin']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def determine_begin_token(cas, ment_details) :\n",
    "    \"\"\"\n",
    "    contient calcul distance pour rajouter au df la colonne de distance entre mentions\n",
    "    le valeur indique la distance entre la fin de la mention précédente et le début \n",
    "    de la mention suivante - première mention 0 ? dernière mention ?\n",
    "    \n",
    "    permet de trouver le token de début et de fin des mentions puis\n",
    "    utilise calcul_distance pour rajouter col distance au df \n",
    "\n",
    "    Parameters    \n",
    "    cas : TYPE\n",
    "    ment_details : pandas DataFrame avec détails des mentions \n",
    "    (indice début et fin, étiquette etc.)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ment_details : pandas DataFrame en entrée avec colonne des distances en plus\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    ##compter distance entre tokens\n",
    "    tokens = list(cas.select(token_type))\n",
    "    #mentions = list(cas.select(mention_type))\n",
    "    \n",
    "    #filter out punctuation tokens from the text\n",
    "    non_punct_tokens = [token for token in tokens if getattr(token, 'pos', None).coarseValue != 'PUNCT']\n",
    "    \n",
    "    #à partir des mentions, on calcule le token de début de chaque mention\n",
    "    #listes vides pour indices de début et de fin de la mention    \n",
    "    mention_indices_begin = []\n",
    "    mention_indices_end = []\n",
    "    \n",
    "    # Match tokens with mentions and track indices\n",
    "    for mention in ment_details.itertuples():  # Iterate over rows in ment_details df\n",
    "        begin_index = None\n",
    "        end_index = None\n",
    "        #iterate over tokens in text\n",
    "        for i, token in enumerate(non_punct_tokens):\n",
    "            if token.begin == mention.begin and begin_index is None:\n",
    "                begin_index = i\n",
    "            if token.end == mention.end and end_index is None:\n",
    "                end_index = i\n",
    "        \n",
    "        mention_indices_begin.append(begin_index)\n",
    "        mention_indices_end.append(end_index)\n",
    "        \n",
    "    if len(mention_indices_begin) != len(ment_details):\n",
    "        raise ValueError(\"Length mismatch: 'mention_indices_begin' and 'ment_details' row count do not match.\")\n",
    "\n",
    "    if len(mention_indices_end) != len(ment_details):\n",
    "        raise ValueError(\"Length mismatch: 'mention_indices_end' and 'ment_details' row count do not match.\")\n",
    "    \n",
    "    ment_details['tokenBegin'] = mention_indices_begin\n",
    "    ment_details['tokenEnd'] = mention_indices_end\n",
    "    \n",
    "    ment_details = calcul_distance(ment_details)\n",
    "    \n",
    "    return ment_details\n",
    "\n",
    "def position_in_chain(df, tag_column=\"tag\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    associer index à chaque mention correspondant à sa position dans sa chaine\n",
    "    ex. deuxième mention de la chaine \"cat\" = 2\n",
    "    \"\"\"\n",
    "    tag_counts = {}\n",
    "    occurrences = []\n",
    "    \n",
    "    for tags in df[tag_column]:\n",
    "        tag_tuple = tuple(tags) if isinstance(tags, list) else (tags,)\n",
    "        \n",
    "        if tag_tuple not in tag_counts:\n",
    "            tag_counts[tag_tuple] = 1\n",
    "        else:\n",
    "            tag_counts[tag_tuple] += 1\n",
    "        \n",
    "    occurrences.append(tag_counts[tag_tuple])\n",
    "\n",
    "    df[\"tag_occurrences\"] = occurrences\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mentions_by_tag(cas, mention_type_name):\n",
    "    \"\"\"\n",
    "    Regroupe les mentions par étiquette. \n",
    "    Overlapping mentions are counted once for each tag\n",
    "    TODO add overlapping mentions option in dictionary ? how ?\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cas : TYPE\n",
    "        DESCRIPTION.\n",
    "    mention_type_name : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ment_by_tag : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    \"\"\"\n",
    "    mention_type = typesystem.get_type(mention_type_name)\n",
    "    mentions = list(cas.select(mention_type))\n",
    "    \n",
    "    # Organize mentions by tag\n",
    "    ment_by_tag = {}\n",
    "    for mention in mentions:\n",
    "        tag = mention.mention\n",
    "        if tag not in ment_by_tag:\n",
    "            ment_by_tag[tag] = []\n",
    "        ment_by_tag[tag].append(mention)\n",
    "\n",
    "    return ment_by_tag\n",
    "\n",
    "def mentions_per_entity(data_dict, key_order):\n",
    "    \"\"\"\n",
    "    Returns the lengths of the value lists in a dictionary, following a specific order of keys.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dict (dict): The input dictionary where keys map to lists of objects.\n",
    "    - key_order (list): A list specifying the desired order of keys.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lengths corresponding to the key order, with 0 or None for missing keys.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for key in key_order:\n",
    "        if key in data_dict:\n",
    "            result.append(len(data_dict[key]))\n",
    "        else:\n",
    "            result.append(0)  # Change to None if None is preferred\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_tag_occurrence_column(df, tag_column=\"tag\"):\n",
    "    #nuñéroter les mentions meme tag pour ordre\n",
    "    tag_counts = {}\n",
    "    occurrences = []\n",
    "\n",
    "    for tags in df[tag_column]:\n",
    "        tag_tuple = tuple(sorted(tags)) if isinstance(tags, list) else (tags,)\n",
    "\n",
    "        if tag_tuple not in tag_counts:\n",
    "            tag_counts[tag_tuple] = 1\n",
    "        else:\n",
    "            tag_counts[tag_tuple] += 1\n",
    "        \n",
    "        occurrences.append(tag_counts[tag_tuple])\n",
    "    \n",
    "    df[\"tag_occurrences\"] = occurrences\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to convert tags containing commas\n",
    "def convert_tags(tag_str):\n",
    "    # Replace commas with '+'\n",
    "    a = tag_str.replace(\"[\",\"\")\n",
    "    b = a.replace(\"]\",\"\")\n",
    "    c = b.replace(\" \",\"\")\n",
    "    d = c.replace(\"'\",\"\")\n",
    "    return d.replace(',', '+')\n",
    "\n",
    "\n",
    "#before recomposing apply calcul distance on each group        \n",
    "# Function to calculate distances\n",
    "def calcul_interdistance(df):\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "    df = df.reset_index(drop=True)  # Reset index to ensure sequential order\n",
    "    df['interdistance'] = 0  # Initialize distance column\n",
    "\n",
    "    for i in range(1, len(df)):  # Start from the second row\n",
    "    \n",
    "        curr_begin = df.loc[i, 'tokenBegin']\n",
    "        curr_end = df.loc[i, 'tokenEnd']\n",
    "        prev_begin = df.loc[i - 1, 'tokenBegin']\n",
    "        prev_end = df.loc[i - 1, 'tokenEnd']\n",
    "\n",
    "        # Case 1: Distant spans\n",
    "        if curr_begin > prev_end:\n",
    "            df.loc[i, 'interdistance'] = curr_begin - prev_end - 1\n",
    "\n",
    "        # Case 2: Overlapping mentions (nested)\n",
    "        elif curr_begin == prev_begin and curr_end > prev_end:\n",
    "            df.loc[i, 'interdistance'] = 0\n",
    "\n",
    "        # Case 3: Smaller span inside a larger span\n",
    "        elif curr_begin > prev_begin and curr_end < prev_end:\n",
    "            df.loc[i, 'interdistance'] = 0  \n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def unify_csv_files(folder_path, filter_substring=None):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in the specified folder into a single Pandas DataFrame.\n",
    "    Uses the header from the first file and skips the first row for subsequent files.\n",
    "    Adds a column with a substring extracted from the filename.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        filter_substring (str, optional): Substring to filter files by name. \n",
    "                                          Only files containing this substring will be included.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Unified DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(folder_path)\n",
    "    csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "    \n",
    "    # Apply optional filter\n",
    "    if filter_substring:\n",
    "        csv_files = [f for f in csv_files if filter_substring in f]\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No files matched the criteria or the folder is empty.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create an empty list to store DataFrames\n",
    "    dataframes = []\n",
    "    \n",
    "    header = None  # To store the header from the first file\n",
    "    for i, file in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "                \n",
    "        try:\n",
    "            if i == 0:\n",
    "                # Read the first file normally, and set its header\n",
    "                df = pd.read_csv(file_path)\n",
    "                header = df.columns  # Save header from the first file\n",
    "            else:\n",
    "                # For subsequent files, skip the first row\n",
    "                df = pd.read_csv(file_path, skiprows=1, header=None)\n",
    "                df.columns = header  # Assign header to the current DataFrame\n",
    "            \n",
    "            # Add the extracted value as a new column\n",
    "            df['Source'] = file\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    unified_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Step 1: Replace '0' in 'distance' with NaN where 'Index' is 0\n",
    "    unified_df.loc[unified_df['Unnamed: 0'] == 0, 'distance'] = unified_df.loc[unified_df['Unnamed: 0'] == 0, 'distance'].replace(0, np.nan)\n",
    "    return unified_df\n",
    "    \n",
    "    \n",
    "#tagset pour francais et italien complet\n",
    "key_order = [\n",
    "            'cat', 'witch', 'wolf', 'robot', 'ext1', 'ext2', 'ext3', 'ext4', \\\n",
    "            'ext5', 'ext6', 'ext7', 'ext8', 'ext9', 'cat1', 'cat2', 'cat3', \\\n",
    "            'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'robot1', 'robot2', \\\n",
    "            'robot3', 'robot4', 'robot5', 'witch1', 'witch2', 'witch3', \\\n",
    "            'wolf1', 'wolf2', 'wolf3'\n",
    "            ]\n",
    "\n",
    "################  MAIN PROGRAM  ###############################################\n",
    "folder_path = os.path.dirname('./italien/')\n",
    "output_folder = './stats_ita/'\n",
    "\n",
    "\n",
    "all_res=[]\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    results = []\n",
    "    if filename.endswith('.xmi'):\n",
    "        print(\"Elaborating \"+filename)\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        filename = os.path.splitext(filename)[0]\n",
    "        # Load CAS\n",
    "        cas = load_cas_xmi(file_path)\n",
    "        results.append(filename)\n",
    "        \n",
    "        #compter nb tokens\n",
    "        toks = count_tokens(cas)\n",
    "        results.append(toks)\n",
    "        toks_no_punct = count_tokens_nopunct(cas)\n",
    "        results.append(toks_no_punct)\n",
    "        \n",
    "        #creer dataframe 1 per texte qui contient tout détail de mentions\n",
    "        #obtenir un folder avec un fichier csv par texte avec le détail des mentions\n",
    "        mention_details = extract_mention_details(cas, token_type, mention_type)\n",
    "        mention_dataframe = create_mentions_df(mention_details)\n",
    "        mention_dataframe_2 = determine_begin_token(cas, mention_dataframe)\n",
    "        mention_dataframe_3 = add_tag_occurrence_column(mention_dataframe_2)\n",
    "        os.makedirs(output_folder , exist_ok=True)\n",
    "        mention_dataframe_3.to_csv(output_folder+filename+\".csv\", \",\", encoding=\"utf-8\")\n",
    "        \n",
    "        # TODO dataframe avec interdistance meme entité calculée\n",
    "        #todo integrer dans uima cas tt\n",
    "        # Group by 'tag', apply function, and recompose\n",
    "        # Apply the function to convert tags in the 'tag' column\n",
    "        mention_dataframe_3['tag'] = mention_dataframe_3['tag'].astype(str).apply(convert_tags)\n",
    "        grouped_df = mention_dataframe_3.groupby('tag', sort=False)  # Keep original order\n",
    "        \n",
    "        #from tag lists to tag+tag in alphabetical order\n",
    "        processed_groups = [calcul_interdistance(group) for _, group in grouped_df]\n",
    "        recomposed_df = pd.concat(processed_groups).reset_index(drop=True)\n",
    "        os.makedirs('./stats_ita_interdistance/', exist_ok=True)\n",
    "        recomposed_df.to_csv(\"./stats_ita_interdistance/\"+filename+\".csv\", \",\", encoding=\"utf-8\")\n",
    "        print(\"done \"+filename)\n",
    "        \n",
    "    \n",
    "        #trouver les tags à rajouter au df \n",
    "        ment_tags = mentions_by_tag(cas,mention_type_name)    \n",
    "        output = mentions_per_entity(ment_tags, key_order)\n",
    "        results = results + output\n",
    "        prev_header = ['texte', 'nbTok', 'nbTokNoPunct'] + key_order\n",
    "        header = prev_header\n",
    "        all_res.append(results)\n",
    "\n",
    "\n",
    "#dataframe with nbTok (with/noPunct) and mentions per character in text\n",
    "#TODO add how many mentions are plural and what characters are involved?\n",
    "\n",
    "#csv unique avec tout le corpus + colonne nom fichier en dernier\n",
    "# utile seulement pour nb tokens pas pour le reste\n",
    "\n",
    "res = pd.DataFrame(all_res, columns = header)\n",
    "#droppa colonne che contengono solo degli zero ok\n",
    "res.replace(0, np.nan).dropna(axis=1,how=\"all\")\n",
    "res.to_csv(\"./corpus_italien.csv\", \",\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "#count nb of characters per text\n",
    "res['Characters'] = (res.iloc[:, 3:] > 0).sum(axis=1)\n",
    "char_nb = res['Characters'].value_counts()\n",
    "\n",
    "filtered_CE1 = res.loc[res['texte'].str.contains('CE1', na=False), 'Characters'].value_counts()\n",
    "print(filtered_CE1)\n",
    "\n",
    "filtered_CE2 = res.loc[res['texte'].str.contains('CE2', na=False), 'Characters'].value_counts()\n",
    "print(filtered_CE2)\n",
    "\n",
    "\n",
    "# FR folder = './stats_fr_v3/'\n",
    "df = unify_csv_files(output_folder)  # un seul csv depuis folder\n",
    "print(df)\n",
    "df.to_csv(\"./toutes_mentions_corpus_detail_ita.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "folder = './stats_ita_interdistance/'\n",
    "df2 = unify_csv_files(folder)  # un seul csv depuis folder\n",
    "print(df2)\n",
    "df2.to_csv(\"./toutes_mentions_corpus_detail_interdistance_ita.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "nb_texts = len(res['texte'])\n",
    "nb_tokens = res['nbTokNoPunct'].sum()\n",
    "\n",
    "# Supponiamo che il tuo DataFrame sia df\n",
    "\n",
    "# Count occurrences of each tag per source\n",
    "df_pivot = df.groupby(['Source', 'tag']).size().unstack(fill_value=0)\n",
    "\n",
    "# # Numero di colonne che contengono almeno un valore diverso da zero\n",
    "nb_characters = (df_pivot != 0).any(axis=0).sum()\n",
    "\n",
    "nb_mentions = len(df)\n",
    "\n",
    "\n",
    "# Numero di valori maggiori o uguali a 3 nelle colonne dalla quarta in poi\n",
    "chaines = (df_pivot >= 3).sum().sum()\n",
    "\n",
    "# Numero di valori uguali a 2 nelle colonne dalla quarta in poi\n",
    "anaphores = (df_pivot == 2).sum().sum()\n",
    "\n",
    "# Numero di valori uguali a 1 nelle colonne dalla quarta in poi\n",
    "singletons = (df_pivot == 1).sum().sum()\n",
    "\n",
    "# Step 1: Create a new column with the sum of integer values per row\n",
    "df_pivot['all_mentions'] = df_pivot.select_dtypes(include='number').sum(axis=1)\n",
    "\n",
    "df_pivot = df_pivot.reset_index()\n",
    "# Step 2: Sum all values in 'row_sum' where 'texte' contains 'CE1'\n",
    "total_sum_ce1 = df_pivot.loc[df_pivot['Source'].str.contains('CE1', na=False), 'all_mentions'].sum()\n",
    "print(\"Total sum where 'Source' contains 'CE1':\", total_sum_ce1)\n",
    "\n",
    "# Step 2: Sum all values in 'row_sum' where 'texte' contains 'CE1'\n",
    "total_sum_ce2 = df_pivot.loc[df_pivot['Source'].str.contains('CE2', na=False), 'all_mentions'].sum()\n",
    "print(\"Total sum where 'Source' contains 'CE2':\", total_sum_ce2)\n",
    "\n",
    "\n",
    "\n",
    "# Group by 'Source' and 'tag' and perform both operations\n",
    "result = df2.groupby(['Source', 'tag'], as_index=False).agg({\n",
    "    'distance': 'sum',\n",
    "    'tag_occurrences': 'max',\n",
    "})\n",
    "# Creazione della nuova colonna 'interdistance'\n",
    "result['interdistance'] = result['distance'] / result['tag_occurrences']\n",
    "len_moy_chaines = result['tag_occurrences'].mean()\n",
    "result.to_csv(\"./interdistance_ita.csv\", sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "\n",
    "ce1_len_moy = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].mean()\n",
    "ce2_len_moy = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].mean()\n",
    "\n",
    "len_max_chaines = result['tag_occurrences'].max()\n",
    "ce1_len_max = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].max()\n",
    "ce2_len_max = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].max()\n",
    "\n",
    "# moy_chaines_texte = \n",
    "# ce1_len_moy = result.loc[result['Source'].str.contains('CE1', na=False), 'tag_occurrences'].mean()\n",
    "# ce2_len_moy = result.loc[result['Source'].str.contains('CE2', na=False), 'tag_occurrences'].mean()\n",
    "\n",
    "\n",
    "txt = \"./res_ita.txt\"\n",
    "with open(txt, \"w\") as file:\n",
    "    file.write(f\"Nb of texts: {nb_texts}\\n\")\n",
    "    file.write(f\"Nb of tokens: {nb_tokens}\\n\")\n",
    "    file.write(f\"Nb of characters: {nb_characters}\\n\")\n",
    "    file.write(f\"Nb of mentions: {nb_mentions}\\n\")\n",
    "    file.write(f\"Nb of mentions CE1 : {total_sum_ce1}\\n\")\n",
    "    file.write(f\"Nb of mentions CE2 : {total_sum_ce2}\\n\")\n",
    "    \n",
    "    file.write(f\"Chaines  >= 3: {chaines}\\n\")\n",
    "    file.write(f\"Anaphores  = 2: {anaphores}\\n\")\n",
    "    file.write(f\"Singletons = 1: {singletons}\\n\")\n",
    "    file.write(f\"Len moyenne chaines {len_moy_chaines}\\n\")\n",
    "    file.write(f\"Len moyenne chaines CE1 {ce1_len_moy}\\n\")\n",
    "    file.write(f\"Len moyenne chaines CE2 {ce2_len_moy}\\n\")\n",
    "    file.write(f\"Len max chaines {len_max_chaines}\\n\")\n",
    "    file.write(f\"Len max chaines CE1 {ce1_len_max}\\n\")\n",
    "    file.write(f\"Len max chaines CE2 {ce2_len_max}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte {}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte CE1 {}\\n\")\n",
    "    # file.write(f\"Nb moyen chaines par texte CE2 {}\\n\")\n",
    "    file.write(f\"Tot.: {chaines+anaphores+singletons}\\n\")\n",
    "    file.write(f\"\")\n",
    "print(f\"File {txt} has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
